{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 32, 128, 128]) Output is on cuda:0\n",
      "Execution time: 0.008056 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len = 128\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    scale = head_dim ** -0.5\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Output shape:\", output.shape, \"Output is on\", output.device)\n",
    "\n",
    "average_time = (end_time - start_time)/10\n",
    "\n",
    "print(\"Execution time: {:.6f} seconds\".format(average_time))\n",
    "\n",
    "del Q, K, V, scores, attention_weights, output\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192] # 512,1024,2048,4096,\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "    # batch_size = int(16384/seq_len)\n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "    \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "    \n",
    "    end_event.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    result = output.sum()\n",
    "\n",
    "    print(\"Reduced result:\", result.item())\n",
    "\n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "    del Q, K, V, scores, attention_weights, output, result\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMR_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "    # batch_size = int(16384/seq_len)\n",
    "        \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    " \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "          \n",
    "    start_event.record()\n",
    "\n",
    "    # for i in range(10):\n",
    "    \n",
    "    scale = head_dim ** -0.5\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "      \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_weights_dmr = F.softmax(scores, dim=-1)\n",
    "\n",
    "      \n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event) \n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output - attention_weights_dmr.sum()\n",
    "    result = result.sum()\n",
    "   \n",
    "      \n",
    "    print(\"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, attention_weights_dmr\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMR_with check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "     \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "         \n",
    "    start_event.record()\n",
    "\n",
    "    # for i in range(10):\n",
    "     \n",
    "     \n",
    "    scale = head_dim ** -0.5\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "      \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    attention_weights_dmr = F.softmax(attention_weights, dim=-1)\n",
    "    \n",
    "    sum_1 = attention_weights.sum()\n",
    "    sum_2 = attention_weights_dmr.sum()\n",
    "\n",
    "    if torch.equal(sum_1, sum_2):\n",
    "        sum_1 = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        sum_2 = torch.tensor(0.0, device=device)\n",
    "    \n",
    "\n",
    "      \n",
    "    output = torch.matmul(attention_weights_dmr, V)\n",
    "\n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event) \n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(f\"Sum_1: {sum_1.item()}, Sum_2: {sum_2.item()}\")\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, attention_weights_dmr, sum_1, sum_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABFT_GEMM_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "    \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        Q_sum_row = Q.sum(dim=2, keepdim=True)\n",
    "        row_checksum = torch.matmul(Q_sum_row, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        sum_1 = scores.sum()\n",
    "        sum_2 = row_checksum.sum()\n",
    "        if torch.equal(sum_1, sum_2):\n",
    "            sum_1 = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            sum_2 = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(f\"Sum_1: {sum_1.item()}, Sum_2: {sum_2.item()}\", \"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, Q_sum_row, row_checksum, sum_1, sum_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABFT_GEMM1_elementcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "     \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "    sum_1 = torch.tensor(0.0, device=device)\n",
    "    sum_2 = torch.tensor(0.0, device=device)\n",
    "    \n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        Q_sum_row = Q.sum(dim=2, keepdim=True)\n",
    "        row_checksum = torch.matmul(Q_sum_row, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        rowsum = scores.sum(dim=2, keepdim=True)\n",
    "        \n",
    "        if torch.equal(row_checksum, rowsum):\n",
    "            sum_1 = torch.tensor(1.0, device=device)\n",
    "        else:\n",
    "            sum_2 = torch.tensor(1.0, device=device)\n",
    "\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"row_checksum shape:\", row_checksum.shape, \"rowsum shape:\", rowsum.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(f\"Sum_1: {sum_1.item()}, Sum_2: {sum_2.item()}\")\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, Q_sum_row, row_checksum, sum_1, sum_2, rowsum\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABFT_GEMM_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "      \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        V_sum_col = V.sum(dim=3, keepdim=True)\n",
    "        col_checksum = torch.matmul(attention_weights, V_sum_col)\n",
    "\n",
    "        sum_1 = output.sum()\n",
    "        sum_2 = col_checksum.sum()\n",
    "        if torch.equal(sum_1, sum_2):\n",
    "            sum_1 = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            sum_2 = torch.tensor(0.0, device=device)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(f\"Sum_1: {sum_1.item()}, Sum_2: {sum_2.item()}\", \"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, V_sum_col, col_checksum, sum_1, sum_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABFT_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "      \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    \n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        flag = torch.tensor(1.0, device=device)\n",
    "    \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        Q_sum_row = Q.sum(dim=2, keepdim=True)\n",
    "        row_checksum = torch.matmul(Q_sum_row, K.transpose(-1, -2)) * scale\n",
    "\n",
    "        rowsum = scores.sum(dim=2, keepdim=True)\n",
    "        if torch.equal(row_checksum, rowsum):\n",
    "            flag = torch.tensor(1.0, device=device)\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(\"Reduced result:\", result.item(), \"flag: \", flag)\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMR_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096]\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "      \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "      \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights_dmr = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum() + attention_weights_dmr.sum()\n",
    "\n",
    "      \n",
    "    print(\"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, attention_weights_dmr\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the loop order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512,1024,2048,4096,8192] # 512,1024,2048,4096,\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "time_list = {512: 0,\n",
    "             1024: 0,\n",
    "             2048: 0,\n",
    "             4096: 0,\n",
    "             8192: 0}\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(10):\n",
    "    for seq_len in seq_len_set:\n",
    "        # batch_size = int(16384/seq_len)\n",
    "          \n",
    "        Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "        K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "        V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "         \n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "          \n",
    "        start_event.record()\n",
    "\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        \n",
    "          \n",
    "        end_event.record()\n",
    "\n",
    "          \n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "          \n",
    "        elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "        time_list[seq_len] += elapsed_time\n",
    "\n",
    "         \n",
    "        print(\"Output shape:\", output.shape)\n",
    "\n",
    "          \n",
    "        result = output.sum()\n",
    "\n",
    "          \n",
    "        print(\"Reduced result:\", result.item())\n",
    "\n",
    "          \n",
    "        print(\"Execution time: {:.6f} ms\".format(elapsed_time),\"\\n\")\n",
    "\n",
    "          \n",
    "        del Q, K, V, scores, attention_weights, output, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for key, value in time_list.items():\n",
    "    final_time = value/10\n",
    "    print(f\"seq_len={key}; exe_time={final_time:.6f} ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**attention with all ft technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 32, 512, 128])\n",
      "Reduced result: -3020.0\n",
      "Execution time: 6.730676 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "# batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512] # 512,1024,2048,4096,\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "    batch_size = int(16384/seq_len)\n",
    "      \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "          \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum()\n",
    "\n",
    "      \n",
    "    print(\"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 32, 512, 128])\n",
      "Reduced result: 21408.0\n",
      "Execution time: 4.550621 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "  \n",
    "# batch_size = 2\n",
    "num_heads = 32\n",
    "seq_len_set = [512] # 512,1024,2048,4096,\n",
    "head_dim = 128 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seq_len in seq_len_set:\n",
    "    batch_size = int(16384/seq_len)\n",
    "      \n",
    "    Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "    V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "     \n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "      \n",
    "    start_event.record()\n",
    "\n",
    "    for i in range(10):\n",
    "         \n",
    "         \n",
    "        scale = head_dim ** -0.5\n",
    "        K_t = K.transpose(-1, -2)\n",
    "        K_check = K_t.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        scores = torch.matmul(Q, K_t) * scale\n",
    "        scores_check = torch.matmul(Q, K_check) * scale\n",
    "\n",
    "          \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "\n",
    "          \n",
    "        V_check = V.sum(dim=-1, keepdim=True)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        out_check = torch.matmul(attention_weights, V_check)\n",
    "\n",
    "    \n",
    "      \n",
    "    end_event.record()\n",
    "\n",
    "      \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "      \n",
    "    elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "     \n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "      \n",
    "    result = output.sum() + scores_check.sum() + out_check.sum()\n",
    "\n",
    "      \n",
    "    print(\"Reduced result:\", result.item())\n",
    "\n",
    "      \n",
    "    print(\"Execution time: {:.6f} ms\".format(elapsed_time/10),\"\\n\")\n",
    "\n",
    "      \n",
    "    del Q, K, V, scores, attention_weights, output, result, V_check, scores_check, K_check, out_check\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 times average reaults**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -3602.0\n",
      "Execution time: 16.570047 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -768.0\n",
      "Execution time: 2.262400 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: 576.5\n",
      "Execution time: 2.264928 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: 627.5\n",
      "Execution time: 2.264064 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: 273.0\n",
      "Execution time: 2.260288 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -3122.0\n",
      "Execution time: 2.263616 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: 4720.0\n",
      "Execution time: 2.258400 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -6408.0\n",
      "Execution time: 2.264832 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -3050.0\n",
      "Execution time: 2.260544 ms \n",
      "\n",
      "Output shape: torch.Size([32, 16, 512, 64])\n",
      "Reduced result: -2768.0\n",
      "Execution time: 2.261216 ms \n",
      "\n",
      "Seq_len: 512; Average execution time: 3.693034 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def baseline(test_len):\n",
    "      \n",
    "    # batch_size = 2\n",
    "    num_heads = 16\n",
    "    seq_len_set = [test_len] # 512,1024,2048,4096,\n",
    "    head_dim = 64 # hidden_dim = num_head * head_dim\n",
    "\n",
    "\n",
    "      \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for seq_len in seq_len_set:\n",
    "        batch_size = int(16384/seq_len)\n",
    "          \n",
    "        Q = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "        K = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "        V = (torch.rand(batch_size, num_heads, seq_len, head_dim, device=device) * 2 - 1).half()\n",
    "\n",
    "         \n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "          \n",
    "        start_event.record()\n",
    "\n",
    "        for i in range(1):\n",
    "             \n",
    "             \n",
    "            scale = head_dim ** -0.5\n",
    "            scores = torch.matmul(Q, K.transpose(-1, -2)) * scale\n",
    "\n",
    "              \n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "              \n",
    "            output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        \n",
    "          \n",
    "        end_event.record()\n",
    "\n",
    "          \n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "          \n",
    "        elapsed_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "         \n",
    "        print(\"Output shape:\", output.shape)\n",
    "\n",
    "          \n",
    "        result = output.sum()\n",
    "\n",
    "          \n",
    "        print(\"Reduced result:\", result.item())\n",
    "\n",
    "          \n",
    "        print(\"Execution time: {:.6f} ms\".format(elapsed_time),\"\\n\")\n",
    "\n",
    "          \n",
    "        del Q, K, V, scores, attention_weights, output, result\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return elapsed_time\n",
    "\n",
    "\n",
    "total_time = 0\n",
    "for i in range(10):\n",
    "    total_time += baseline(512)\n",
    "print(\"Seq_len: 512; Average execution time: {:.6f} ms\".format(total_time/10),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
